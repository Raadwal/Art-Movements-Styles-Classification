{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O93BMLCI_SR"
      },
      "outputs": [],
      "source": [
        "!pip install deeplake[enterprise]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTmfej0aInWM"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import deeplake\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G0ns5IwuLXC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u86REDSpNaqf"
      },
      "outputs": [],
      "source": [
        "token = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVPC5_EKJA_B"
      },
      "outputs": [],
      "source": [
        "train_ds = deeplake.load('hub://um_project/art-train', token=token, read_only=True)\n",
        "dev_ds = deeplake.load('hub://um_project/art-dev', token=token, read_only=True)\n",
        "val_ds = deeplake.load('hub://um_project/art-val', token=token, read_only=True)\n",
        "test_ds = deeplake.load('hub://um_project/art-test', token=token, read_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHzstNa7JI-F"
      },
      "outputs": [],
      "source": [
        "print(f'Size of train dataset: {len(train_ds)}')\n",
        "print(f'Size of dev dataset: {len(dev_ds)}')\n",
        "print(f'Size of validation dataset: {len(val_ds)}')\n",
        "print(f'Size of test dataset: {len(test_ds)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gll9FKPuJK1A"
      },
      "outputs": [],
      "source": [
        "classes_labels = train_ds.labels.info.class_names\n",
        "num_classes = len(classes_labels)\n",
        "print(f'Number of classes: {num_classes}')\n",
        "for i, label in enumerate(classes_labels):\n",
        "  print(f'{i}. {label}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnQReYTFOx-m"
      },
      "outputs": [],
      "source": [
        "def plot_class_distribution(ax, class_counts, class_labels, dataset_name):\n",
        "    ax.bar(np.arange(len(class_labels)), class_counts, tick_label=class_labels)\n",
        "    ax.set_xlabel('Class', weight='bold')\n",
        "    ax.set_xticklabels(class_labels, rotation='vertical')\n",
        "    ax.set_ylabel('Number of Instances', weight='bold')\n",
        "    ax.set_title(f'Frequency per Class ({dataset_name})', weight='bold')\n",
        "\n",
        "class_train_count = np.bincount(np.concatenate(train_ds.labels.numpy(aslist = True), axis=0))\n",
        "class_dev_count = np.bincount(np.concatenate(dev_ds.labels.numpy(aslist = True), axis=0))\n",
        "class_val_count = np.bincount(np.concatenate(val_ds.labels.numpy(aslist = True), axis=0))\n",
        "class_test_count = np.bincount(np.concatenate(test_ds.labels.numpy(aslist = True), axis=0))\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 12), constrained_layout=True)\n",
        "\n",
        "plot_class_distribution(axs[0, 0], class_train_count, classes_labels, \"Train\")\n",
        "plot_class_distribution(axs[0, 1], class_dev_count, classes_labels, \"Dev\")\n",
        "plot_class_distribution(axs[1, 0], class_val_count, classes_labels, \"Val\")\n",
        "plot_class_distribution(axs[1, 1], class_test_count, classes_labels, \"Test\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O6NxqpqAc4QP"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to store images for each class\n",
        "class_images = {label: [] for label in classes_labels}\n",
        "\n",
        "# Iterate through the dataset and store 7 images for each class\n",
        "for i, sample in enumerate(train_ds):\n",
        "    label = sample['labels'].data()['text'][0]  # Access the first element of the list\n",
        "    if len(class_images[label]) < 7:\n",
        "        image_array = sample['images'].data()['value']\n",
        "        class_images[label].append(image_array)\n",
        "        \n",
        "    # Stop iterating if we already have 7 images for each class\n",
        "    if all(len(images) == 7 for images in class_images.values()):\n",
        "        break\n",
        "\n",
        "\n",
        "# Create a grid of subplots and display the images\n",
        "num_classes = len(classes_labels)\n",
        "num_images_per_class = 7\n",
        "fig = plt.figure(figsize=(28, 56))\n",
        "\n",
        "for i, class_label in enumerate(classes_labels):\n",
        "    for j, image_array in enumerate(class_images[class_label]):\n",
        "        ax = fig.add_subplot(num_classes, num_images_per_class, i * num_images_per_class + j + 1)\n",
        "        ax.imshow(image_array)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Set the class name above the first image in each row\n",
        "        if j == int(num_images_per_class / 2):\n",
        "            ax.set_title(class_label, fontsize=32, ha='center', va='center', weight='bold')\n",
        "\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyaZLuWjKOG0"
      },
      "outputs": [],
      "source": [
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet statistics\n",
        "])\n",
        "\n",
        "def one_hot_encode(label, num_classes):\n",
        "    one_hot = torch.zeros(num_classes)\n",
        "    one_hot[label] = 1\n",
        "    return one_hot\n",
        "\n",
        "batch_size = 64\n",
        "num_workers = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Odp3bnmkiYAL"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(dataset, batch_size, num_classes, image_transform, shuffle=True, num_workers=0):\n",
        "    return dataset.pytorch(\n",
        "        num_workers=num_workers,\n",
        "        batch_size=batch_size,\n",
        "        transform={\n",
        "            'images': image_transform,\n",
        "            'labels': lambda label: one_hot_encode(label, num_classes),\n",
        "        },\n",
        "        shuffle=shuffle,\n",
        "        decode_method={'images': 'pil'}\n",
        "    )\n",
        "\n",
        "# Create data loaders for different datasets\n",
        "train_loader = create_data_loader(train_ds, batch_size, num_classes, image_transform)\n",
        "dev_loader = create_data_loader(dev_ds, batch_size, num_classes, image_transform)\n",
        "val_loader = create_data_loader(val_ds, batch_size, num_classes, image_transform)\n",
        "test_loader = create_data_loader(test_ds, batch_size, num_classes, image_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jct4UGAGTJKN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "num_classes = 13\n",
        "resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, num_classes)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Model is curretly running on: {device}\")\n",
        "resnet18.to(device)\n",
        "\n",
        "# Set the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(resnet18.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gudY_y9gucsb"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, epoch, save_path, model_name):\n",
        "  # Create the save directory if it doesn't exist\n",
        "  if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "  # Create the full path for the saved model\n",
        "  model_file = os.path.join(save_path, f\"{model_name}_epoch_{epoch}.pth\")\n",
        "\n",
        "  # Save the model and optimizer state_dicts\n",
        "  torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "  }, model_file)\n",
        "\n",
        "  print(f\"Model saved: {model_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tQY7um_ud9z"
      },
      "outputs": [],
      "source": [
        "def load_model(model, optimizer, load_path, device):\n",
        "  # Load the saved model and optimizer state_dicts\n",
        "  checkpoint = torch.load(load_path)\n",
        "\n",
        "  # Load the model and optimizer state_dicts into the model and optimizer objects\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  # Move the model to the appropriate device (GPU or CPU)\n",
        "  model.to(device)\n",
        "\n",
        "  # Set the starting epoch for the model\n",
        "  start_epoch = checkpoint['epoch']\n",
        "\n",
        "  print(f\"Model loaded: {load_path}, starting from epoch {start_epoch}\")\n",
        "\n",
        "# Usage example:\n",
        "#load_path = \"/content/drive/MyDrive/SSN_Projekt/Saved_Models/MultiLabelCNN_epoch_1.pth\"\n",
        "#load_model(model, optimizer, load_path, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK8GVYuCXs4K"
      },
      "outputs": [],
      "source": [
        "def train_validate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience):\n",
        "    \n",
        "    save_path = \"/content/drive/MyDrive/UM_Projekt/Saved_Models\" \n",
        "    model_name = \"Resnet\"\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    best_model = None\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data['images'].to(device), data['labels'].to(device)\n",
        "\n",
        "            # Convert one-hot encoded labels to class indices\n",
        "            labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_epoch_loss = running_loss / (i + 1)\n",
        "        train_epoch_accuracy = correct / total * 100\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader, 0):\n",
        "                inputs, labels = data['images'].to(device), data['labels'].to(device)\n",
        "\n",
        "                # Convert one-hot encoded labels to class indices\n",
        "                labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = running_loss / (i + 1)\n",
        "        val_epoch_accuracy = correct / total * 100\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_epoch_loss:.4f}, Training Accuracy: {train_epoch_accuracy:.2f}%, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.2f}%\")\n",
        "\n",
        "        # Save the model after each epoch\n",
        "        save_model(model, optimizer, epoch + 1, save_path, model_name)\n",
        "\n",
        "        # Save the best model and implement early stopping\n",
        "        if val_epoch_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_epoch_accuracy\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "            counter = 0\n",
        "\n",
        "            save_model(model, optimizer, epoch + 1, save_path, f\"{model_name}_Best\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}. Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(best_model)\n",
        "    return model, best_val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6zOYpEiwjso"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            inputs, labels = data['images'].to(device), data['labels'].to(device)\n",
        "\n",
        "            # Convert one-hot encoded labels to class indices\n",
        "            labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct / total * 100\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    return test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTLYmWRbVr3R"
      },
      "source": [
        "#Hyperparameter OptiMiser Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oERL1i7iyMDA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def optiMiser_hyperparams(dev_loader, val_loader, test_loader, train_model_fun, test_model_fun, param_grid, num_epochs, device, patience):\n",
        "    best_params = None\n",
        "    best_score = 0.0\n",
        "\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        #reset model, if there is better way then tell me\n",
        "        resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        num_classes = 13\n",
        "        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, num_classes)\n",
        "\n",
        "        # Set device\n",
        "        resnet18.to(device)\n",
        "\n",
        "        model = resnet18\n",
        "\n",
        "        optimizer = getattr(optim, params['optimizer'])(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "        criterion = params['criterion']\n",
        "        print(f\"Starting evaluation for: {params}\")\n",
        "\n",
        "        trained_model, val_score = train_model_fun(model=model, train_loader=dev_loader, val_loader = val_loader, criterion=criterion, optimizer=optimizer, device = device, num_epochs=num_epochs, patience = patience)\n",
        "        \n",
        "        \n",
        "        if val_score > best_score:\n",
        "            best_score = val_score\n",
        "            best_params = params\n",
        "\n",
        "        print(\"\")\n",
        "        print(f\"Finished evaluation for: {params}\")\n",
        "        print(f\"Test Accuracy: {val_score:.2f}%\")\n",
        "        print(f\"Best Accuracy: {best_score:.2f}%\")\n",
        "        print(\"\")\n",
        "\n",
        "    return best_params, best_score\n",
        "\n",
        "param_score_array = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Znvnic_1oEZ"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'lr' : [0.001],\n",
        "    'optimizer': ['SGD', 'Adam', 'RMSprop'],\n",
        "    'weight_decay': [0.0001, 0.001, 0.01],\n",
        "    'criterion': [torch.nn.CrossEntropyLoss(), torch.nn.NLLLoss(), torch.nn.BCELoss()]\n",
        "}\n",
        "#param_grid = {\n",
        "#    'lr' : [0.001],\n",
        "#    'optimizer': ['SGD', 'Adam', 'RMSprop'],\n",
        "#    'weight_decay': [0.0001, 0.001, 0.01],\n",
        "#    'criterion': [torch.nn.CrossEntropyLoss(), torch.nn.NLLLoss(), torch.nn.BCELoss()]\n",
        "#}\n",
        "num_epochs = 1000\n",
        "patience = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fUOXucuy7mZ"
      },
      "outputs": [],
      "source": [
        "best_params, best_score = optiMiser_hyperparams(dev_loader, val_loader, test_loader, train_validate, test_model, param_grid, num_epochs, device, patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPNxVzyToxip"
      },
      "outputs": [],
      "source": [
        "#{'batch_size', 'lr', 'optimizer', 'weight_decay'}\n",
        "#Best = [1, 1, 2, 1] #58.73%\n",
        "#Furthest = [1, 1, 2, 1]\n",
        "#Furthest_aNEW [1,1,2,1] #only SGD\n",
        "param_score_array.append(tuple([best_params, best_score]))\n",
        "\n",
        "for i in range(len(param_score_array)):\n",
        "  print(param_score_array[i][0], \" - \", param_score_array[i][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc8B1wnxV0Jg"
      },
      "source": [
        "#Hyperparameter OptiMiser End"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URVAhOrBkIh0"
      },
      "outputs": [],
      "source": [
        "# Set the number of epochs and patience\n",
        "num_epochs = 1000\n",
        "patience = 5\n",
        "\n",
        "# Train and validate the model with early stopping\n",
        "best_model, best_val_accuracy = train_validate(resnet18, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F3PRnhFwwoS"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/MyDrive/UM_Projekt/Saved_Models\" \n",
        "model_name = \"Resnet\"\n",
        "best_epoch = 5\n",
        "\n",
        "best_model_path = os.path.join(save_path, f\"{model_name}_Best_epoch_{best_epoch}.pth\")\n",
        "load_model(resnet18, optimizer, best_model_path, device)\n",
        "\n",
        "test_accuracy = test_model(resnet18, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HjZ4qqYyuBr"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kH-tYFD-zHx"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "batch = next(iter(test_loader))\n",
        "images, _ = batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbWGE6fhCudN"
      },
      "outputs": [],
      "source": [
        "background = images[:50].to(device)\n",
        "test_images = images[50:55].to(device)\n",
        "\n",
        "e = shap.DeepExplainer(resnet18, background)\n",
        "shap_values = e.shap_values(test_images)\n",
        "\n",
        "shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
        "test_numpy = np.swapaxes(np.swapaxes(test_images.cpu().numpy(), 1, -1), 1, 2)\n",
        "\n",
        "def normalize_data(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "test_numpy = normalize_data(test_numpy)\n",
        "\n",
        "shap.image_plot(shap_numpy, test_numpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJBPDRZBDB6I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}